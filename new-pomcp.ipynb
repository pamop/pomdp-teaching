{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new pomcp from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new pomcp from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime # for limiting calculation to wall clock time\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # game class\n",
    "class TigerProblem():\n",
    "    def __init__(self,obs_truth = 0.8):\n",
    "#         self.state = ('tiger-left' if random.random() < 0.5 else 'tiger-right')\n",
    "        self.obs_truth = obs_truth # how much you can trust observations\n",
    "        self.all_states = set(['tiger-left','tiger-right'])\n",
    "        \n",
    "    # this contains the transition function for the MDP\n",
    "    def next_state(self, state, action):\n",
    "        \n",
    "        assert action in ['listen','open-left','open-right'], \"invalid action\"\n",
    "        \n",
    "        if action == 'listen':\n",
    "            return state\n",
    "        elif action == 'open-left':\n",
    "            return ('tiger-left' if random.random() < 0.5 else 'tiger-right')\n",
    "        elif action == 'open-right':            \n",
    "            return ('tiger-left' if random.random() < 0.5 else 'tiger-right')\n",
    "       \n",
    "    # observation function for the POMDP\n",
    "    def observation(self, state, action):  \n",
    "        \n",
    "        assert action in ['listen','open-left','open-right'], \"invalid action\"\n",
    "        \n",
    "        all_s = self.all_states\n",
    "        \n",
    "        if action == 'listen':\n",
    "            if random.random() < self.obs_truth:\n",
    "                return state\n",
    "            else:\n",
    "                other = [st for st in self.all_states if st != state]\n",
    "                return other[0]\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    # Take a sequence of game states representing the full game tree, and return the full list\n",
    "    # of actions that are legal actions\n",
    "    def legal_actions(self,state_hist): \n",
    "        return ['listen','open-left','open-right']\n",
    "    \n",
    "    # Should this be the length of state_tree (how long tama alive for)? or is it trial by trial rwd?\n",
    "    def reward(self, state, action):\n",
    "        if action == 'listen':\n",
    "            return -1\n",
    "        elif action == 'open-left':\n",
    "            return -20 if state == 'tiger-left' else 10\n",
    "        elif action == 'open-right':            \n",
    "            return -20 if state == 'tiger-right' else 10\n",
    "            \n",
    "    # GENERATOR MODEL OF GAME\n",
    "    # returns next state, observation, and reward given an action taken in given state\n",
    "    # takes tuple state\n",
    "    def G_model(self,state,action):\n",
    "        s = self.next_state(state,action)\n",
    "        obs = self.observation(state,action)\n",
    "        rwd = self.reward(state,action) # note that this should be more like immediate reward of state, not long-term?\n",
    "        done = False\n",
    "        return s, obs, rwd, done\n",
    "    \n",
    "    # Initial state distribution\n",
    "    # Initial state distribution\n",
    "    def sample_prior(self):\n",
    "        s = ('tiger-left' if random.random() < 0.5 else 'tiger-right')\n",
    "        return s\n",
    "    \n",
    "    # when filtering particles, this is the rule to keep one given a real observation\n",
    "    def keep_particle(self, part, real_obs):\n",
    "        trash_prob = 0.8\n",
    "        if real_obs == []:\n",
    "            return True\n",
    "        if part != real_obs and random.random() < trash_prob:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    #     if real_obs == []:\n",
    "    #         return True\n",
    "    #     else:\n",
    "    #         if part == real_obs:\n",
    "    #             return True\n",
    "    #     return False\n",
    "\n",
    "    # generate a new particle from one randomly sampled from current belief (e.g., just add a lil noise)\n",
    "    def new_particle(self, part):\n",
    "        s = ('tiger-left' if random.random() < 0.5 else 'tiger-right')\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchTree(object):\n",
    "    def __init__(self,visits=1,value=0):\n",
    "        self.visits = visits\n",
    "        self.value = value\n",
    "        self.children=[]\n",
    "        \n",
    "        \n",
    "class ActionNode(SearchTree):\n",
    "    def __init__(self,action=None,visits=1,value=0):\n",
    "        super().__init__(visits,value)\n",
    "        self.action = action\n",
    "        \n",
    "        \n",
    "class ObservationNode(SearchTree):\n",
    "    def __init__(self,observation=[],visits=1,value=0,belief=[]):\n",
    "        super().__init__(visits,value)\n",
    "        self.observation = observation\n",
    "        self.belief = belief\n",
    "        \n",
    "    def expand(self, legal_actions):\n",
    "        for a in legal_actions:\n",
    "            self.children += [ActionNode(a)]\n",
    "            \n",
    "        # upper confidence bound value for given node \"child\"\n",
    "    def ucb(self, child): #maybe use index of child not object\n",
    "#         print(\"self.visit=\",self.visit,\" len of self.children=\",len(self.children))\n",
    "        logval = math.log(self.visits) #, len(self.children))\n",
    "        div = logval / child.visits\n",
    "        return math.sqrt(div)\n",
    "    \n",
    "#     def sample_belief(self):\n",
    "#         return random.choice(self.belief)\n",
    "    \n",
    "    def next_hist(self,action,obs):\n",
    "        act_child = next((c for c in self.children if c.action==action), None)\n",
    "        assert act_child != None, \"shouldn't you be expanded already?\"\n",
    "        assert isinstance(act_child, ActionNode), \"action child should be an action node!\"\n",
    "        \n",
    "        obs_child = next((c for c in act_child.children if c.observation == obs), None) \n",
    "        if obs_child is None:\n",
    "            act_child.children += [ObservationNode(obs)]\n",
    "#             print(act_child.children[0].observation)\n",
    "            obs_child = next((c for c in act_child.children if c.observation == obs), None)     \n",
    "        return obs_child\n",
    "    \n",
    "#     def next_hist_rollout(self,action,obs):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POMCP:\n",
    "    def __init__(self, \n",
    "                 game=TigerProblem(),\n",
    "                 discount=0.8,\n",
    "                 epsilon=1e-7,\n",
    "                 explore=1,\n",
    "                 n_particles=100,\n",
    "                 reinvigoration=20, \n",
    "                 **kwargs):\n",
    "        \n",
    "#         self.context = {}\n",
    "        self.game = game\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "        self.explore = explore\n",
    "        self.n_particles = n_particles\n",
    "        self.reinvigoration = reinvigoration\n",
    "        self.G = game.G_model      \n",
    "        self.tree = None\n",
    "        self.history = []\n",
    "        # list of all possible actions\n",
    "#         self.actions = kwargs.get('actions') \n",
    "        \n",
    "        seconds = kwargs.get('time',30)\n",
    "        self.calculation_time = datetime.timedelta(seconds=seconds)\n",
    "        self.maxdepth = kwargs.get('maxdepth',20)\n",
    "        self.nsims = kwargs.get('nsims',1000)\n",
    "        \n",
    "    def search(self,obs):\n",
    "        \n",
    "        self.history += [obs]\n",
    "        \n",
    "        if self.tree is None:\n",
    "            self.tree = ObservationNode(obs)                        \n",
    "#             particle = self.game.sample_prior()\n",
    "#             self.simulate(particle,self.tree,0)\n",
    "        else:\n",
    "            self.prune_tree(obs)\n",
    "            \n",
    "        for _ in range(self.nsims):\n",
    "            particle = self.draw_sample()\n",
    "            self.simulate(particle,self.tree,0)\n",
    "        \n",
    "        child = self.greedy_action_selection(self.tree,self.game.legal_actions(self.tree)) # will again need to handle legal actions differently for real\n",
    "        self.tree = child # move forward to child action node (will move to obs node when real obs occurs)\n",
    "        self.history += [child.action]\n",
    "        \n",
    "        return child.action\n",
    "    \n",
    "    def simulate(self,state,tree,depth):\n",
    "        if depth >= self.maxdepth:\n",
    "            return 0\n",
    "        \n",
    "#         legal = self.game.legal_actions(state,tree,depth)\n",
    "        legal = self.game.legal_actions(tree) # would want it to be more elegant/complicated for real\n",
    "    \n",
    "        if len(tree.children) == 0:\n",
    "            tree.expand(legal)\n",
    "            return self.rollout(state,depth)\n",
    "        \n",
    "        if len(legal)==1:\n",
    "            action = legal[0]\n",
    "            child = tree.children[0]\n",
    "        else:\n",
    "            child = self.ucb_action_selection(tree,legal)\n",
    "            action = child.action\n",
    "            \n",
    "        next_state, next_obs, r, done = self.G(state,action)\n",
    "        next_tree = tree.next_hist(action,next_obs)\n",
    "        reward = r + self.discount * self.simulate(next_state,next_tree,depth+1)\n",
    "        \n",
    "        tree.belief += [state] \n",
    "        tree.visits += 1\n",
    "        \n",
    "        child.visits += 1\n",
    "        child.value += (reward - child.value)/child.visits\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def rollout(self,state,depth):\n",
    "        if depth >= self.maxdepth:\n",
    "            return 0\n",
    "        \n",
    "        legal = self.game.legal_actions([\"whatever but change this later\"])\n",
    "        a = random.choice(legal)\n",
    "        \n",
    "        next_state, next_obs, r, done = self.G(state,a)\n",
    "#         next_tree = tree.next_hist(a,next_obs)\n",
    "        \n",
    "        if done:\n",
    "            return r\n",
    "        \n",
    "        return r + self.discount * self.rollout(next_state,depth+1)\n",
    "        \n",
    "    def prune_tree(self,obs):\n",
    "        #current tree is an action node. find child node with observation obs\n",
    "        obs_child = next((c for c in self.tree.children if c.observation == obs), None) \n",
    "        self.tree = obs_child\n",
    "        return\n",
    "        \n",
    "    def greedy_action_selection(self,tree,legal):\n",
    "        children = [child for child in tree.children if child.action in legal] #filter(lambda child: child.action in legal_actions, tree.children)\n",
    "        child_vals = np.array([child.value for child in children])\n",
    "        favechildren = np.argwhere(child_vals == np.amax(child_vals))\n",
    "        child = children[random.choice(favechildren.flatten().tolist())]\n",
    "        return child\n",
    "        \n",
    "    def ucb_action_selection(self,tree,legal):\n",
    "        children = [child for child in tree.children if child.action in legal] #filter(lambda child: child.action in legal_actions, tree.children)\n",
    "        child_vals = np.array([child.value + self.explore * tree.ucb(child) for child in children])\n",
    "        favechildren = np.argwhere(child_vals == np.amax(child_vals))\n",
    "        child = children[random.choice(favechildren.flatten().tolist())]\n",
    "        return child\n",
    "    \n",
    "    def draw_sample(self):\n",
    "        if self.tree.belief == []:\n",
    "            return self.game.sample_prior()\n",
    "        else:\n",
    "            return random.choice(self.tree.belief)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ('tiger-left' if random.random() < 0.5 else 'tiger-right')\n",
    "game = TigerProblem()\n",
    "agent = POMCP(game, 0.9, maxdepth=20, nsims=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL STATE: \n",
      "tiger-right\n",
      "Taking action: listen\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the tree runs simulations to choose the next action using the choose_move() method\n",
    "\n",
    "print(\"INITIAL STATE: \")\n",
    "print(s) # initial tiger problem state\n",
    "\n",
    "obs = []\n",
    "\n",
    "action = agent.search(obs)\n",
    "print(\"Taking action:\", action)\n",
    "state = game.next_state(s,action)\n",
    "game.reward(s,action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['left belief:', 8340]\n",
      "['right belief:', 9065]\n",
      "['left belief percentage:', 0.4791726515369147]\n"
     ]
    }
   ],
   "source": [
    "# if next obs is tiger-left,\n",
    "observeright = 1 #1 if observes right on next observation, 0 if observes left\n",
    "nparticles = len(agent.tree.children[observeright].belief)\n",
    "nleftbelief = len([b for b in agent.tree.children[observeright].belief if b=='tiger-left'])\n",
    "nrightbelief = len([b for b in agent.tree.children[observeright].belief if b=='tiger-right'])\n",
    "print(['left belief:',nleftbelief])\n",
    "print(['right belief:',nrightbelief])\n",
    "print(['left belief percentage:',nleftbelief/nparticles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so clearly i still need to limit how many particles are added to each observation node but also like, \n",
    "# this doesn't seem right, the beliefs should be different given the real observation that happens next\n",
    "# ALSO, when there are many many many more possible actions i might not really need to limit it that much\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL STATE: \n",
      "tiger-left\n",
      "Action 1: True state is tiger-left\n",
      "Taking action listen.\n",
      "observed  tiger-left\n",
      "Reward so far:  -1\n",
      "Action 2: True state is tiger-left\n",
      "Taking action open-right.\n",
      "Reward so far:  9\n",
      "Action 3: True state is tiger-left\n",
      "Taking action listen.\n",
      "observed  tiger-left\n",
      "Reward so far:  8\n",
      "Action 4: True state is tiger-left\n",
      "Taking action listen.\n",
      "observed  tiger-left\n",
      "Reward so far:  7\n",
      "Action 5: True state is tiger-left\n",
      "Taking action listen.\n",
      "observed  tiger-left\n",
      "Reward so far:  6\n",
      "Action 6: True state is tiger-left\n",
      "Taking action open-right.\n",
      "Reward so far:  16\n",
      "Action 7: True state is tiger-left\n",
      "Taking action listen.\n",
      "observed  tiger-left\n",
      "Reward so far:  15\n",
      "Action 8: True state is tiger-left\n",
      "Taking action open-right.\n",
      "Reward so far:  25\n",
      "Action 9: True state is tiger-right\n",
      "Taking action listen.\n",
      "observed  tiger-right\n",
      "Reward so far:  24\n",
      "Action 10: True state is tiger-right\n",
      "Taking action open-left.\n",
      "Reward so far:  34\n",
      "Action 11: True state is tiger-left\n",
      "Taking action open-right.\n",
      "Reward so far:  44\n",
      "Action 12: True state is tiger-right\n",
      "Taking action listen.\n",
      "observed  tiger-left\n",
      "Reward so far:  43\n",
      "Action 13: True state is tiger-right\n",
      "Taking action listen.\n",
      "observed  tiger-right\n",
      "Reward so far:  42\n",
      "Action 14: True state is tiger-right\n",
      "Taking action open-left.\n",
      "Reward so far:  52\n",
      "Action 15: True state is tiger-right\n",
      "Taking action open-right.\n",
      "Reward so far:  32\n",
      "Action 16: True state is tiger-right\n",
      "Taking action listen.\n",
      "observed  tiger-right\n",
      "Reward so far:  31\n",
      "Action 17: True state is tiger-right\n",
      "Taking action listen.\n",
      "observed  tiger-left\n",
      "Reward so far:  30\n",
      "Action 18: True state is tiger-right\n",
      "Taking action listen.\n",
      "observed  tiger-right\n",
      "Reward so far:  29\n",
      "Action 19: True state is tiger-right\n",
      "Taking action listen.\n",
      "observed  tiger-right\n",
      "Reward so far:  28\n",
      "Action 20: True state is tiger-right\n",
      "Taking action listen.\n",
      "observed  tiger-right\n",
      "Reward so far:  27\n",
      "Action 21: True state is tiger-right\n",
      "Taking action open-left.\n",
      "Reward so far:  37\n",
      "game over!\n"
     ]
    }
   ],
   "source": [
    "# Now, let the POMCP do its thing for several actions in a row\n",
    "\n",
    "# Initialize the tiger problem\n",
    "s = ('tiger-left' if random.random() < 0.5 else 'tiger-right')\n",
    "game = TigerProblem() #obs_truth=0.95)\n",
    "    \n",
    "print(\"INITIAL STATE: \")\n",
    "print(s) # initial tiger state\n",
    "\n",
    "agent = POMCP(game, 0.9, maxdepth=20, nsims=1000)\n",
    "\n",
    "action_seq = []\n",
    "state = s\n",
    "obs = []\n",
    "R = 0\n",
    "\n",
    "\n",
    "while len(action_seq) <= 20: # play for a certain amount of time (better rule?)\n",
    "\n",
    "    action = agent.search(obs)\n",
    "    action_seq.append(action)\n",
    "    print('Action %i: True state is %s'% (len(action_seq), state))\n",
    "    print(\"Taking action %s.\"% action)\n",
    "    \n",
    "    obs = game.observation(state,action)\n",
    "    if obs!=[]:\n",
    "        print(\"observed \",obs)\n",
    "\n",
    "    r = game.reward(state,action)\n",
    "    R = R + r\n",
    "    print(\"Reward so far: \",R)    \n",
    "    \n",
    "    state = game.next_state(state,action)\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"game over!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
