{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new pomcp from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new pomcp from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime # for limiting calculation to wall clock time\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # game class\n",
    "class TigerProblem():\n",
    "    def __init__(self,obs_truth = 0.8):\n",
    "#         self.state = ('tiger-left' if random.random() < 0.5 else 'tiger-right')\n",
    "        self.obs_truth = obs_truth # how much you can trust observations\n",
    "        self.all_states = set(['tiger-left','tiger-right'])\n",
    "        \n",
    "    # this contains the transition function for the MDP\n",
    "    def next_state(self, state, action):\n",
    "        \n",
    "        assert action in ['listen','open-left','open-right'], \"invalid action\"\n",
    "        \n",
    "        if action == 'listen':\n",
    "            return state\n",
    "        elif action == 'open-left':\n",
    "            return ('tiger-left' if random.random() < 0.5 else 'tiger-right')\n",
    "        elif action == 'open-right':            \n",
    "            return ('tiger-left' if random.random() < 0.5 else 'tiger-right')\n",
    "       \n",
    "    # observation function for the POMDP\n",
    "    def observation(self, state, action):  \n",
    "        \n",
    "        assert action in ['listen','open-left','open-right'], \"invalid action\"\n",
    "        \n",
    "        all_s = self.all_states\n",
    "        \n",
    "        if action == 'listen':\n",
    "            if random.random() < self.obs_truth:\n",
    "                return state\n",
    "            else:\n",
    "                other = [st for st in self.all_states if st != state]\n",
    "                return other[0]\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    # Take a sequence of game states representing the full game tree, and return the full list\n",
    "    # of actions that are legal actions\n",
    "    def legal_actions(self,state_hist): \n",
    "        return ['listen','open-left','open-right']\n",
    "    \n",
    "    # Should this be the length of state_tree (how long tama alive for)? or is it trial by trial rwd?\n",
    "    def reward(self, state, action):\n",
    "        if action == 'listen':\n",
    "            return -1\n",
    "        elif action == 'open-left':\n",
    "            return -20 if state == 'tiger-left' else 10\n",
    "        elif action == 'open-right':            \n",
    "            return -20 if state == 'tiger-right' else 10\n",
    "            \n",
    "    # GENERATOR MODEL OF GAME\n",
    "    # returns next state, observation, and reward given an action taken in given state\n",
    "    # takes tuple state\n",
    "    def G_model(self,state,action):\n",
    "        s = self.next_state(state,action)\n",
    "        obs = self.observation(state,action)\n",
    "        rwd = self.reward(state,action) # note that this should be more like immediate reward of state, not long-term?\n",
    "        done = False\n",
    "        return s, obs, rwd, done\n",
    "    \n",
    "    # Initial state distribution\n",
    "    # Initial state distribution\n",
    "    def sample_prior(self):\n",
    "        s = ('tiger-left' if random.random() < 0.5 else 'tiger-right')\n",
    "        return s\n",
    "    \n",
    "    # when filtering particles, this is the rule to keep one given a real observation\n",
    "    def keep_particle(self, part, real_obs):\n",
    "        trash_prob = 0.8\n",
    "        if real_obs == []:\n",
    "            return True\n",
    "        if part != real_obs and random.random() < trash_prob:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    #     if real_obs == []:\n",
    "    #         return True\n",
    "    #     else:\n",
    "    #         if part == real_obs:\n",
    "    #             return True\n",
    "    #     return False\n",
    "\n",
    "    # generate a new particle from one randomly sampled from current belief (e.g., just add a lil noise)\n",
    "    def new_particle(self, part):\n",
    "        s = ('tiger-left' if random.random() < 0.5 else 'tiger-right')\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchTree(object):\n",
    "    def __init__(self,visits=1,value=0):\n",
    "        self.visits = visits\n",
    "        self.value = value\n",
    "        self.children=[]\n",
    "        \n",
    "        \n",
    "class ActionNode(SearchTree):\n",
    "    def __init__(self,action=None,visits=1,value=0):\n",
    "        super().__init__(visits,value)\n",
    "        self.action = action\n",
    "        \n",
    "        \n",
    "class ObservationNode(SearchTree):\n",
    "    def __init__(self,observation=[],visits=1,value=0,belief=[]):\n",
    "        super().__init__(visits,value)\n",
    "        self.observation = observation\n",
    "        self.belief = belief\n",
    "        \n",
    "    def expand(self, legal_actions):\n",
    "        for a in legal_actions:\n",
    "            self.children += [ActionNode(a)]\n",
    "            \n",
    "        # upper confidence bound value for given node \"child\"\n",
    "    def ucb(self, child): #maybe use index of child not object\n",
    "#         print(\"self.visit=\",self.visit,\" len of self.children=\",len(self.children))\n",
    "        logval = math.log(self.visits) #, len(self.children))\n",
    "        div = logval / child.visits\n",
    "        return math.sqrt(div)\n",
    "    \n",
    "#     def sample_belief(self):\n",
    "#         return random.choice(self.belief)\n",
    "    \n",
    "    def next_hist(self,action,obs):\n",
    "        act_child = next((c for c in self.children if c.action==action), None)\n",
    "        assert act_child != None, \"shouldn't you be expanded already?\"\n",
    "        assert isinstance(act_child, ActionNode), \"action child should be an action node!\"\n",
    "        \n",
    "        obs_child = next((c for c in act_child.children if c.observation == obs), None) \n",
    "        if obs_child is None:\n",
    "            act_child.children += [ObservationNode(obs)]\n",
    "#             print(act_child.children[0].observation)\n",
    "            obs_child = next((c for c in act_child.children if c.observation == obs), None)     \n",
    "        return obs_child\n",
    "    \n",
    "#     def next_hist_rollout(self,action,obs):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POMCP:\n",
    "    def __init__(self, \n",
    "                 game=TigerProblem(),\n",
    "                 discount=0.8,\n",
    "                 epsilon=1e-7,\n",
    "                 explore=1,\n",
    "                 n_particles=100,\n",
    "                 reinvigoration=20, \n",
    "                 **kwargs):\n",
    "        \n",
    "#         self.context = {}\n",
    "        self.game = game\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "        self.explore = explore\n",
    "        self.n_particles = n_particles\n",
    "        self.reinvigoration = reinvigoration\n",
    "        self.G = game.G_model      \n",
    "        self.tree = None\n",
    "        self.history = []\n",
    "        # list of all possible actions\n",
    "#         self.actions = kwargs.get('actions') \n",
    "        \n",
    "        seconds = kwargs.get('time',30)\n",
    "        self.calculation_time = datetime.timedelta(seconds=seconds)\n",
    "        self.maxdepth = kwargs.get('maxdepth',20)\n",
    "        self.nsims = kwargs.get('nsims',1000)\n",
    "        \n",
    "    def search(self,obs):\n",
    "        \n",
    "        self.history += [obs]\n",
    "        \n",
    "        if self.tree is None:\n",
    "            self.tree = ObservationNode(obs)                        \n",
    "#             particle = self.game.sample_prior()\n",
    "#             self.simulate(particle,self.tree,0)\n",
    "        else:\n",
    "            self.prune_tree(obs)\n",
    "            \n",
    "        for _ in range(self.nsims):\n",
    "            particle = self.draw_sample()\n",
    "            self.simulate(particle,self.tree,0)\n",
    "        \n",
    "        child = self.greedy_action_selection(self.tree,)\n",
    "        self.tree = child # move forward to child action node (will move to obs node when real obs occurs)\n",
    "        self.history += [child.action]\n",
    "        \n",
    "        return child.action\n",
    "    \n",
    "    def simulate(self,state,tree,depth):\n",
    "        if depth >= self.maxdepth:\n",
    "            return 0\n",
    "        \n",
    "#         legal = self.game.legal_actions(state,tree,depth)\n",
    "        legal = self.game.legal_actions(tree) # would want it to be more elegant/complicated for real\n",
    "    \n",
    "        if len(tree.children) == 0:\n",
    "            tree.expand(legal)\n",
    "            return self.rollout(state,depth)\n",
    "        \n",
    "        if len(legal)==1:\n",
    "            action = legal[0]\n",
    "            child = tree.children[0]\n",
    "        else:\n",
    "            child = self.ucb_action_selection(tree,legal)\n",
    "            action = child.action\n",
    "            \n",
    "        next_state, next_obs, r, done = self.G(state,action)\n",
    "        next_tree = tree.next_hist(action,next_obs)\n",
    "        reward = r + self.discount * self.simulate(next_state,next_tree,depth+1)\n",
    "        \n",
    "        tree.belief += [state] \n",
    "        tree.visits += 1\n",
    "        \n",
    "        child.visits += 1\n",
    "        child.value += (reward - child.value)/child.visits\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def rollout(self,state,depth):\n",
    "        if depth >= self.maxdepth:\n",
    "            return 0\n",
    "        \n",
    "        legal = self.game.legal_actions([\"whatever but change this later\"])\n",
    "        a = random.choice(legal)\n",
    "        \n",
    "        next_state, next_obs, r, done = self.G(state,a)\n",
    "#         next_tree = tree.next_hist(a,next_obs)\n",
    "        \n",
    "        if done:\n",
    "            return r\n",
    "        \n",
    "        return r + self.discount * self.rollout(next_state,depth+1)\n",
    "        \n",
    "    def prune_tree(self,obs):\n",
    "        #current tree is an action node. find child node with observation obs\n",
    "        obs_child = next((c for c in self.tree.children if c.observation == obs), None) \n",
    "        self.tree = obs_child\n",
    "        return\n",
    "        \n",
    "    def greedy_action_selection(self,tree,legal):\n",
    "        children = [child for child in tree.children if child.action in legal] #filter(lambda child: child.action in legal_actions, tree.children)\n",
    "        child_vals = np.array([child.value for child in children])\n",
    "        favechildren = np.argwhere(child_vals == np.amax(child_vals))\n",
    "        child = children[random.choice(favechildren.flatten().tolist())]\n",
    "        return child\n",
    "        \n",
    "    def ucb_action_selection(self,tree,legal):\n",
    "        children = [child for child in tree.children if child.action in legal] #filter(lambda child: child.action in legal_actions, tree.children)\n",
    "        child_vals = np.array([child.value + self.explore * tree.ucb(child) for child in children])\n",
    "        favechildren = np.argwhere(child_vals == np.amax(child_vals))\n",
    "        child = children[random.choice(favechildren.flatten().tolist())]\n",
    "        return child\n",
    "    \n",
    "    def draw_sample(self):\n",
    "        if self.tree.belief == []:\n",
    "            return self.game.sample_prior()\n",
    "        else:\n",
    "            return random.choice(self.tree.belief)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ('tiger-left' if random.random() < 0.5 else 'tiger-right')\n",
    "game = TigerProblem()\n",
    "agent = POMCP(game, 0.9, maxdepth=20, nsims=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL STATE: \n",
      "tiger-right\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "greedy_action_selection() missing 1 required positional argument: 'legal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-76426a02bfad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Taking action:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-0d5d4938f457>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreedy_action_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;31m# move forward to child action node (will move to obs node when real obs occurs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: greedy_action_selection() missing 1 required positional argument: 'legal'"
     ]
    }
   ],
   "source": [
    "# Check that the tree runs simulations to choose the next action using the choose_move() method\n",
    "\n",
    "print(\"INITIAL STATE: \")\n",
    "print(s) # initial tiger problem state\n",
    "\n",
    "obs = []\n",
    "\n",
    "action = agent.search(obs)\n",
    "print(\"Taking action:\", action)\n",
    "state = game.next_state(s,action)\n",
    "game.reward(s,action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= ['state']\n",
    "a == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
