{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to POMDPs\n",
    "  \n",
    "pam osborn popp pamop@nyu.edu   \n",
    "feb 2018  \n",
    "\n",
    "##### rough outline:\n",
    "1. MDP vs. POMDP vs. HMM\n",
    "    - little chart of markov chain, MDP, POMDP, HMM\n",
    "2. How we visualize POMDP solutions\n",
    "    - belief spaces associated with actions \n",
    "3. Examples\n",
    "    - Tiger Problem\n",
    "        - Problem description (states, actions, observations, cost/rwd function)\n",
    "        - POMDP solution via value iteration (what solver to use?)\n",
    "        - Simulation of trained model performing the task\n",
    "    - Simple Teaching Problem\n",
    "4. How POMDPs might be useful/necessary in the HMM project\n",
    "    - Discussion of \"Faster teaching via POMDP planning\" Rafferty et al 2016 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP vs. POMDP vs. HMM\n",
    "\n",
    "As Anthony Cassandra at [www.pomdp.org](http://www.pomdp.org/faq.html) puts it,\n",
    "\n",
    "depending on whether we have control over the state transitions, and whether those states are observable or unobservable,\n",
    "\n",
    "| Markov models | No control | Yes control |\n",
    "|---|---|---|\n",
    "| **States observable** | Markov Chain | MDP |\n",
    "|    **Unobservable**   | HMM | POMDP |\n",
    "\n",
    "### MDP vs. POMDP\n",
    "- In an MDP, the states are completely observable\n",
    "    - At any point in time, we know the current state with 100% accuracy\n",
    "- In a POMDP, the states are partially observable\n",
    "    - We have an additional tool, \"observations,\" to provide evidence as to the possible current state\n",
    "        - I suppose we would call these \"emissions\" in the HMM\n",
    "    - We have to maintain a probability distribution over possible states (like in an HMM)\n",
    "\n",
    "### HMM vs. POMDP\n",
    "- In both, states are not directly observable\n",
    "- However, in a POMDP, we have control over the state transitions\n",
    "- Teaching = attempting to control transitions of learner's knowledge/memory states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us make a cute tiger problem POMDP ourselves just to see if we can! (And by we I mean me)\n",
    "\n",
    "nsteps = 20\n",
    "\n",
    "discount=0.95 # weight on rewards earlier in lifetime\n",
    "states = ['tiger-left','tiger-right']\n",
    "actions = ['listen','open-left','open-right'] # Listen (0), open left (1), open right (2)\n",
    "observations = ['tiger-left','tiger-right']\n",
    "\n",
    "start = [0.5,0.5] # prob of being in each state at start\n",
    "\n",
    "#prob of switching given action, state; t = transitions[a][s]\n",
    "tprobs = [[[1.0,0.0],[0.0,1.0]],[[0.5,0.5],[0.5,0.5]],[[0.5,0.5],[0.5,0.5]]] \n",
    "\n",
    "#prob of observing tiger L/R given state; listen-result = obs[a][s]\n",
    "oprobs = [[[0.85,0.15],[0.15,0.85]],[[0.5,0.5],[0.5,0.5]],[[0.5,0.5],[0.5,0.5]]] \n",
    "\n",
    "value = [[-1,-1],[-100,10],[10,-100]]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.random.choice(states, p=start)\n",
    "time = 0\n",
    "b = start # initial belief state\n",
    "\n",
    "def chooseaction(b):\n",
    "    # b = belief state\n",
    "    # insert solved policy here - ideally solve this first and then use policy to simulate playing game\n",
    "    \n",
    "    \n",
    "    \n",
    "while time < nsteps:\n",
    "    print(\"Step \" + str(time+1) + \":\")\n",
    "    a = chooseaction(b) # value iteration, e.g., to choose action\n",
    "    print(\"Agent chose action \" + actions(a))\n",
    "    if a == \"listen\":\n",
    "        \n",
    "    \n",
    "    time+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 1\n"
     ]
    }
   ],
   "source": [
    "print(\"time \" + str(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
