{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search for Teaching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Tree Search (MCTS) is a popular method for determining good decision strategies in large decision spaces (i.e., those with a large branching factor).  This notebook is a little example code for learning the basics of the algorithm.  It draws heavily from the tutorial by Jeff Bradberry published here: https://jeffbradberry.com/posts/2015/09/intro-to-monte-carlo-tree-search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import datetime # for limiting calculation to wall clock time\n",
    "from math import log, sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**How to turn this into a teaching situation?**\n",
    "\n",
    "The `Student` class below gives the basic structure of this teacher-student interaction.  The teacher model provides instructions to the student which influences the state of the student.  The state is directly observable.  The `Student` class is just a model in the sense that it is a fiction in the mind of the teacher.  The student might not exactly behave this way.  However, if the student *did* act this way then the MCTS planning algorithm should be able to decide what the best sequence of teaching actions is.\n",
    "\n",
    "1. **Uncooperative student in a grid world**: The student is an agent in a grid world.  Objective is to reach a particular goal state for a large positive reward.  What instruction should the teacher give?  Assume additionally the student isn't perfect so might not follow directions perfectly... The teacher should anticipate this. (Does the teacher understand the maze or no? No means give instructions and simply observes the outcomes and decides the policy directly during planning.)\n",
    "\n",
    "1. **Growing a plant**: The student is a plant.  Objective is to keep alive until it reaches fruiting stage.  One action is ignore the plant (which can actually be good).  Other's are like apply water, apply fertilizer, apply sunlight, etc...  It is kind of like the two player game because you don't know what the other agent will do.  No partial observability issues here, yet, technically because you can just look at the plant height (let's say).\n",
    "\n",
    "1. **Transparent sequence learner**:  The student is a person learning a digit string.  Objective is to get the student to know the sequence.  Actions are <?>.  Agent can forget.  No partial observability because each time you do a teaching episode the student gives a complete read-out of their current memory (as the state) so you always know where you are in terms of the student's knowledge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Student(object):\n",
    "    \n",
    "    def start(self):\n",
    "        # Returns a representation of the starting state of the game.\n",
    "        pass\n",
    "\n",
    "    def next_state(self, state, instruction):\n",
    "        # Takes the game state, and the move to be applied.\n",
    "        # Returns the new game state.\n",
    "        pass\n",
    "\n",
    "    def teaching_actions(self, state_history):\n",
    "        # Takes a sequence of learners states representing the full\n",
    "        # teaching history, and returns the full list of actions that\n",
    "        # are legal teaching actions\n",
    "        # Question: why does the full teaching histoy influence the actions?\n",
    "        # Answer: in games like checkers the possible moves limited by past plays.  not an issue\n",
    "        # in all problems.\n",
    "        pass\n",
    "\n",
    "    def reward(self, state_history):\n",
    "        # Takes a sequence of learner states representing the full\n",
    "        # teaching history.  If the \"game\" is now won, return a large\n",
    "        #positive reward. If the game is still ongoing, return zero.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The uncooperative student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uncooperative student starts at a random spot on a $N$x$M$ maze (see sepecific maze below).  The goal is to get to a goal state while avoiding obstacles.  Actions are expressed as instructions we give to the student (and the student therefore interpret the actions and may not always follow them).  The action space is \"move up\", \"move down\", \"move left\", and \"move right\".  The actions are available in all states at all times BUT if you run into an obsticle such as a wall or another block then your action simply returns you to the same state.\n",
    "\n",
    "<img src=\"images/gridworld.png\" width=\"300\">\n",
    "\n",
    "In this particular instance of the uncooperative student, the student has a tendency to move down no matter what advice it gets ($\\epsilon$ probability of following impulse and ignoring advice).  I tried to abstract this definition somewhat in the code so you could change this to be more of an intolerable student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    \n",
    "    def __init__(self, gridmap):\n",
    "        self.gridmap = gridmap\n",
    "        self.gridmap_flat = [item for sublist in gridmap for item in sublist]\n",
    "        self.nrows = len(self.gridmap)\n",
    "        self.ncols = len(self.gridmap[0])\n",
    "        self.all_states = [] # includes all states, indexable\n",
    "        self.all_states_rev = {}\n",
    "        self.valid_states = {} # don't include impossible start states\n",
    "        idx = 0\n",
    "        for i in range(self.nrows):\n",
    "            for j in range(self.ncols):\n",
    "                self.all_states.append((i,j))\n",
    "                self.all_states_rev[(i,j)]=idx\n",
    "                idx += 1\n",
    "                if self.gridmap[i][j] == 'o':\n",
    "                    self.valid_states[idx]=(i,j)\n",
    "\n",
    "    def coord_to_index(self, coord):\n",
    "        return self.all_states_rev[coord]\n",
    "    \n",
    "    def index_to_coord(self, index):\n",
    "        return self.all_states[index]\n",
    "    \n",
    "    def raw_print(self):\n",
    "        for i in range(self.nrows):\n",
    "            for j in range(self.ncols):\n",
    "                print(\"%s\"%self.gridmap[i][j],end='\\t')\n",
    "            print (\"\\n\")\n",
    "\n",
    "    def index_print(self):\n",
    "        for i in range(self.nrows):\n",
    "            for j in range(self.ncols):\n",
    "                print(\"(%s,%s)\"%(i,j),end=' ')\n",
    "            print (\"\\n\")\n",
    "\n",
    "    def coord_print(self):\n",
    "        for i in range(self.nrows):\n",
    "            for j in range(self.ncols):\n",
    "                print(\"%s\"%self.coord_to_index((i,j)),end=' ')\n",
    "            print (\"\\n\")\n",
    "\n",
    "\n",
    "    def up(self, state):\n",
    "        i,j = self.index_to_coord(state)\n",
    "        # if in top row just stay where you are\n",
    "        # OR if you'll hit a wall\n",
    "        # OR if you are a wall or goal state (do nothing)\n",
    "        if (i==0) or self.gridmap[i-1][j]=='x' or self.gridmap[i][j]=='x' or self.gridmap[i][j]=='g':\n",
    "            return self.coord_to_index((i,j))\n",
    "        else:\n",
    "            return self.coord_to_index((i-1,j))\n",
    "\n",
    "    def down(self, state):\n",
    "        i,j = self.index_to_coord(state)\n",
    "        # if in bottom row just stay where you are\n",
    "        # OR if you'll hit a wall\n",
    "        # OR if you are a wall or goal state(do nothing)\n",
    "        if (i==self.nrows-1) or self.gridmap[i+1][j]=='x' or self.gridmap[i][j]=='x' or self.gridmap[i][j]=='g':\n",
    "            return self.coord_to_index((i,j))\n",
    "        else:\n",
    "            return self.coord_to_index((i+1,j))\n",
    "    \n",
    "    def left(self, state):\n",
    "        i,j = self.index_to_coord(state)\n",
    "        # if in left-most column just stay where you are\n",
    "        # OR if you'll hit a wall\n",
    "        # OR if you are a wall or goal state (do nothing)\n",
    "        if (j==0) or self.gridmap[i][j-1]=='x' or self.gridmap[i][j]=='x' or self.gridmap[i][j]=='g':\n",
    "            return self.coord_to_index((i,j))\n",
    "        else:\n",
    "            return self.coord_to_index((i,j-1))\n",
    "\n",
    "    def right(self, state):\n",
    "        i,j = self.index_to_coord(state)\n",
    "        # if in right-most column just stay where you are\n",
    "        # OR if you'll hit a wall\n",
    "        # OR if you are a wall or goal state (do nothing)\n",
    "        if (j==self.ncols-1) or self.gridmap[i][j+1]=='x' or self.gridmap[i][j]=='x' or self.gridmap[i][j]=='g':\n",
    "            return self.coord_to_index((i,j))\n",
    "        else:\n",
    "            return self.coord_to_index((i,j+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\to\to\to\to\to\to\tx\tg\t\n",
      "\n",
      "o\to\tx\to\to\to\to\tx\to\t\n",
      "\n",
      "o\to\tx\to\to\to\to\tx\to\t\n",
      "\n",
      "o\to\tx\to\to\to\to\to\to\t\n",
      "\n",
      "o\to\to\to\to\tx\to\to\to\t\n",
      "\n",
      "o\to\to\to\to\to\to\to\to\t\n",
      "\n",
      "(0,0) (0,1) (0,2) (0,3) (0,4) (0,5) (0,6) (0,7) (0,8) \n",
      "\n",
      "(1,0) (1,1) (1,2) (1,3) (1,4) (1,5) (1,6) (1,7) (1,8) \n",
      "\n",
      "(2,0) (2,1) (2,2) (2,3) (2,4) (2,5) (2,6) (2,7) (2,8) \n",
      "\n",
      "(3,0) (3,1) (3,2) (3,3) (3,4) (3,5) (3,6) (3,7) (3,8) \n",
      "\n",
      "(4,0) (4,1) (4,2) (4,3) (4,4) (4,5) (4,6) (4,7) (4,8) \n",
      "\n",
      "(5,0) (5,1) (5,2) (5,3) (5,4) (5,5) (5,6) (5,7) (5,8) \n",
      "\n",
      "0 1 2 3 4 5 6 7 8 \n",
      "\n",
      "9 10 11 12 13 14 15 16 17 \n",
      "\n",
      "18 19 20 21 22 23 24 25 26 \n",
      "\n",
      "27 28 29 30 31 32 33 34 35 \n",
      "\n",
      "36 37 38 39 40 41 42 43 44 \n",
      "\n",
      "45 46 47 48 49 50 51 52 53 \n",
      "\n",
      "[(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (5, 0), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (5, 7), (5, 8)]\n",
      "{(0, 0): 0, (0, 1): 1, (0, 2): 2, (0, 3): 3, (0, 4): 4, (0, 5): 5, (0, 6): 6, (0, 7): 7, (0, 8): 8, (1, 0): 9, (1, 1): 10, (1, 2): 11, (1, 3): 12, (1, 4): 13, (1, 5): 14, (1, 6): 15, (1, 7): 16, (1, 8): 17, (2, 0): 18, (2, 1): 19, (2, 2): 20, (2, 3): 21, (2, 4): 22, (2, 5): 23, (2, 6): 24, (2, 7): 25, (2, 8): 26, (3, 0): 27, (3, 1): 28, (3, 2): 29, (3, 3): 30, (3, 4): 31, (3, 5): 32, (3, 6): 33, (3, 7): 34, (3, 8): 35, (4, 0): 36, (4, 1): 37, (4, 2): 38, (4, 3): 39, (4, 4): 40, (4, 5): 41, (4, 6): 42, (4, 7): 43, (4, 8): 44, (5, 0): 45, (5, 1): 46, (5, 2): 47, (5, 3): 48, (5, 4): 49, (5, 5): 50, (5, 6): 51, (5, 7): 52, (5, 8): 53}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: (0, 0),\n",
       " 2: (0, 1),\n",
       " 3: (0, 2),\n",
       " 4: (0, 3),\n",
       " 5: (0, 4),\n",
       " 6: (0, 5),\n",
       " 7: (0, 6),\n",
       " 10: (1, 0),\n",
       " 11: (1, 1),\n",
       " 13: (1, 3),\n",
       " 14: (1, 4),\n",
       " 15: (1, 5),\n",
       " 16: (1, 6),\n",
       " 18: (1, 8),\n",
       " 19: (2, 0),\n",
       " 20: (2, 1),\n",
       " 22: (2, 3),\n",
       " 23: (2, 4),\n",
       " 24: (2, 5),\n",
       " 25: (2, 6),\n",
       " 27: (2, 8),\n",
       " 28: (3, 0),\n",
       " 29: (3, 1),\n",
       " 31: (3, 3),\n",
       " 32: (3, 4),\n",
       " 33: (3, 5),\n",
       " 34: (3, 6),\n",
       " 35: (3, 7),\n",
       " 36: (3, 8),\n",
       " 37: (4, 0),\n",
       " 38: (4, 1),\n",
       " 39: (4, 2),\n",
       " 40: (4, 3),\n",
       " 41: (4, 4),\n",
       " 43: (4, 6),\n",
       " 44: (4, 7),\n",
       " 45: (4, 8),\n",
       " 46: (5, 0),\n",
       " 47: (5, 1),\n",
       " 48: (5, 2),\n",
       " 49: (5, 3),\n",
       " 50: (5, 4),\n",
       " 51: (5, 5),\n",
       " 52: (5, 6),\n",
       " 53: (5, 7),\n",
       " 54: (5, 8)}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridworld = [\n",
    "       [ 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'x', 'g'],\n",
    "       [ 'o', 'o', 'x', 'o', 'o', 'o', 'o', 'x', 'o'],\n",
    "       [ 'o', 'o', 'x', 'o', 'o', 'o', 'o', 'x', 'o'],\n",
    "       [ 'o', 'o', 'x', 'o', 'o', 'o', 'o', 'o', 'o'],\n",
    "       [ 'o', 'o', 'o', 'o', 'o', 'x', 'o', 'o', 'o'],\n",
    "       [ 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
    "    ]\n",
    "\n",
    "mygrid = GridWorld(gridworld)\n",
    "mygrid.raw_print()  # print out the grid world\n",
    "mygrid.index_print() # print out the indicies of each state\n",
    "mygrid.coord_print() # print out the coordinates\n",
    "\n",
    "print(mygrid.all_states) # all tuples states as a flat list\n",
    "print(mygrid.all_states_rev) # maps from tuples to indicies\n",
    "mygrid.valid_states # hash from indicies to tuples of non terminal, non-barrier states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UncooperativeStudent(object):\n",
    "\n",
    "    def __init__(self, gridworld, epsilon):\n",
    "        self.world = GridWorld(gridworld)\n",
    "        self.EPS = epsilon\n",
    "\n",
    "    def start(self, s=None):\n",
    "        # choose an initially random state\n",
    "        if not s:\n",
    "            start = random.choice(list(self.world.valid_states.keys()))\n",
    "        else:\n",
    "            start = s\n",
    "        print(\"Starting at state %s:%s\"%(start,self.world.all_states[start]))\n",
    "        return start\n",
    "\n",
    "    def next_state_agent(self, state, action):\n",
    "        # Takes the game state, and the move to be applied.\n",
    "        # Returns the new game state.\n",
    "        if action=='up':\n",
    "            return self.world.up(state)\n",
    "        elif action=='down':\n",
    "            return self.world.down(state)\n",
    "        elif action=='left':\n",
    "            return self.world.left(state)\n",
    "        elif action=='right':\n",
    "            return self.world.right(state)\n",
    "        else:\n",
    "            raise Exception(\"Invalid instruction\")\n",
    "\n",
    "\n",
    "    def next_state(self, state, instruction):\n",
    "        # Takes the game state, and the move to be applied.\n",
    "        # Returns the new game state.\n",
    "        action = self.choice(instruction)\n",
    "        return self.next_state_agent(state, action)\n",
    "\n",
    "    def choice(self, instruction):\n",
    "        if random.random() < self.EPS:\n",
    "            return 'up'\n",
    "        else:\n",
    "            return instruction\n",
    "        \n",
    "    def teaching_actions(self, state_history):\n",
    "        # Takes a sequence of learners states representing the full\n",
    "        # teaching history, and returns the full list of actions that\n",
    "        # are legal teaching actions\n",
    "        # Question: why does the full teaching history influence the actions in an MDP?\n",
    "        # this applies to games where the actions might change based on the game\n",
    "        # state\n",
    "        return ['up', 'down', 'left', 'right']\n",
    "\n",
    "    def reward(self, state_history):\n",
    "        # Takes a sequence of learner states representing the full\n",
    "        # teaching history.  If the \"game\" is now won, return a large\n",
    "        #positive reward. If the game is still ongoing, return zero.\n",
    "        reward = 0\n",
    "        done = False\n",
    "        for h in state_history[1:]:\n",
    "            if self.world.gridmap_flat[h]!='g':\n",
    "                reward += -1\n",
    "            else:\n",
    "                reward += 25\n",
    "                done = True\n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 \n",
      "\n",
      "9 10 11 12 13 14 15 16 17 \n",
      "\n",
      "18 19 20 21 22 23 24 25 26 \n",
      "\n",
      "27 28 29 30 31 32 33 34 35 \n",
      "\n",
      "36 37 38 39 40 41 42 43 44 \n",
      "\n",
      "45 46 47 48 49 50 51 52 53 \n",
      "\n",
      "Starting at state 45:(5, 0)\n",
      "45\n",
      "53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-2, False)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bill = UncooperativeStudent(gridworld, 0.1)\n",
    "bill.world.coord_print() \n",
    "s = bill.start()\n",
    "print(s)\n",
    "print(bill.next_state(53, 'right'))\n",
    "\n",
    "bill.reward([45, 46, 47])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-carlo Tree Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MCTS(object):\n",
    "    \n",
    "    def __init__(self, student, **kwargs):\n",
    "        # takes an instance of a Board and optionally some keyword\n",
    "        # arguments. initializes the list of game states and the\n",
    "        # statistics tables\n",
    "\n",
    "        self.student = student\n",
    "        self.states = []\n",
    "        seconds = kwargs.get('time', 30)\n",
    "        self.max_moves = kwargs.get('max_moves', 100)\n",
    "        self.C = kwargs.get('C', 1.4)\n",
    "        self.calculation_time = datetime.timedelta(seconds=seconds)\n",
    "        self.rewards = {}\n",
    "        self.plays = {}\n",
    "    \n",
    "    def update(self, state):\n",
    "        # takes a game state, and appends it to the history\n",
    "        self.states.append(state)\n",
    "    \n",
    "    def get_play(self):\n",
    "        # causes the AI to calculate the best move from the \n",
    "        # current game state and return it\n",
    "        self.max_depth = 0\n",
    "        state = self.states[-1]\n",
    "        legal = self.student.teaching_actions(self.states[:])\n",
    "        \n",
    "        if not legal:  # none of this is needed now because actions size is fixed\n",
    "            return\n",
    "        if len(legal)==1:\n",
    "            return legal[0]\n",
    "        \n",
    "        games = 0\n",
    "        begin = datetime.datetime.utcnow()\n",
    "        while (datetime.datetime.utcnow() - begin) < self.calculation_time:\n",
    "            self.run_simulation()\n",
    "            games+=1\n",
    "        # useful for debugging\n",
    "        #self.print_tree()\n",
    "\n",
    "\n",
    "        \n",
    "        # statistics here in terms of next states assume this is \n",
    "        # deterministic and not a q-value.\n",
    "        moves_states = [(state, a) for a in legal]\n",
    "        \n",
    "        # display the number of calls of `run_simulation` and the time elapsed\n",
    "        print(games, datetime.datetime.utcnow() - begin)\n",
    "        \n",
    "        # pick the move with the highest average reward\n",
    "        percent_wins, move = max(\n",
    "            (self.rewards.get((s,a), 0) / self.plays.get((s,a), 1), a) \n",
    "            for s, a in moves_states\n",
    "        )\n",
    "        \n",
    "        # display the stats for each possible play\n",
    "        for x in sorted(\n",
    "            ((100 * self.rewards.get((s,a), 0) / self.plays.get((s,a), 1),\n",
    "             self.rewards.get((s,a), 0), self.plays.get((s,a), 0), \n",
    "             a)\n",
    "             for s,a in moves_states),\n",
    "            reverse=True\n",
    "        ):\n",
    "            print(\"{3}: {0:.2f}% ({1} / {2})\".format(*x))\n",
    "    \n",
    "        print(\"Maximum depth search:\", self.max_depth)\n",
    "        \n",
    "        return move\n",
    "    \n",
    "    def print_tree(self):\n",
    "        \n",
    "        board = self.student.world\n",
    "        \n",
    "        for i in range(len(board.all_states)):\n",
    "            print(board.all_states[i], ' ', \n",
    "                  self.rewards.get((i, 'up'),0), ' ',\n",
    "                  self.plays.get((i, 'up'),0),\n",
    "\n",
    "                  self.rewards.get((i, 'down'),0), ' ',\n",
    "                  self.plays.get((i, 'down'),0),\n",
    "\n",
    "                  self.rewards.get((i, 'left'),0), ' ',\n",
    "                  self.plays.get((i, 'left'),0),\n",
    "                  \n",
    "                  self.rewards.get((i, 'right'),0), ' ',\n",
    "                  self.plays.get((i, 'right'),0),\n",
    "                 )\n",
    "            \n",
    "        \n",
    "    def run_simulation(self):\n",
    "        # plays out a \"random\" game from the current position,\n",
    "        # then updates the statistics tables with the result.\n",
    "        plays, rewards = self.plays, self.rewards # for speed\n",
    "        \n",
    "        visited_qs = set()\n",
    "        states_copy = self.states[:]\n",
    "        state = states_copy[-1]\n",
    "\n",
    "        expand = True  # you only expand once\n",
    "        for t in range(self.max_moves):\n",
    "            legal = self.student.teaching_actions(states_copy) # get a valid action\n",
    "            \n",
    "            moves = [(state, a) for a in legal]\n",
    "            \n",
    "            if all(plays.get((s,a)) for s,a in moves):\n",
    "                # if we have stats on all the legal move, use them\n",
    "                log_total = 2.0*log(\n",
    "                    sum(plays[(s,a)] for s,a in moves)\n",
    "                )\n",
    "                \n",
    "                # value of best\n",
    "                value, ins = max(\n",
    "                    ((rewards[(s,a)] / plays[(s,a)]) +\n",
    "                     self.C * sqrt(log_total / plays[(s,a)]),a)\n",
    "                    for s,a in moves\n",
    "                )\n",
    "            else:\n",
    "                ins = random.choice(legal) # choose one\n",
    "                            \n",
    "            if expand and (state,ins) not in self.plays: # if expanding and this is new\n",
    "                expand = False # stop the expansion in this run\n",
    "                self.plays[(state,ins)]=0 # initialize\n",
    "                self.rewards[(state,ins)]=0\n",
    "                if t> self.max_depth:\n",
    "                    self.max_depth = t\n",
    "                    \n",
    "            visited_qs.add((state,ins)) # add this state as visited\n",
    "            \n",
    "            state = self.student.next_state(state, ins) # get next state\n",
    "            states_copy.append(state) # record\n",
    "\n",
    "            reward, done = self.student.reward(states_copy) # compute reward if any\n",
    "            #print(states_copy, reward, done)\n",
    "            if done: # if done then top this simulation\n",
    "                break\n",
    "                \n",
    "        #print(visited_states, reward)\n",
    "        for q in visited_qs: # for each visited state\n",
    "            if q not in self.plays: # if state not in the table of statistics yet\n",
    "                continue\n",
    "            self.plays[q]+=1 #  increase places\n",
    "            self.rewards[q]+=reward # add up the reward you got\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 \n",
      "\n",
      "9 10 11 12 13 14 15 16 17 \n",
      "\n",
      "18 19 20 21 22 23 24 25 26 \n",
      "\n",
      "27 28 29 30 31 32 33 34 35 \n",
      "\n",
      "36 37 38 39 40 41 42 43 44 \n",
      "\n",
      "45 46 47 48 49 50 51 52 53 \n",
      "\n",
      "Starting at state 18:(2, 0)\n",
      "Starting at state 18\n",
      "577 0:00:01.000126\n",
      "up: -10000.00% (-14700 / 147)\n",
      "right: -10000.00% (-14700 / 147)\n",
      "left: -10000.00% (-14700 / 147)\n",
      "down: -10000.00% (-14700 / 147)\n",
      "Maximum depth search: 10\n",
      "Taking action up now at 9\n",
      "452 0:00:01.000732\n",
      "up: -10071.52% (-15913 / 158)\n",
      "right: -10071.97% (-15813 / 157)\n",
      "left: -10071.97% (-15813 / 157)\n",
      "down: -10071.97% (-15813 / 157)\n",
      "Maximum depth search: 4\n",
      "Taking action up now at 0\n",
      "424 0:00:01.001136\n",
      "right: -10129.61% (-15397 / 152)\n",
      "left: -10160.00% (-15240 / 150)\n",
      "up: -10160.26% (-15342 / 151)\n",
      "down: -10160.67% (-15241 / 150)\n",
      "Maximum depth search: 8\n",
      "Taking action right now at 1\n",
      "530 0:00:01.001350\n",
      "right: -10213.44% (-18997 / 186)\n",
      "up: -10254.10% (-18765 / 183)\n",
      "left: -10255.19% (-18767 / 183)\n",
      "down: -10255.19% (-18767 / 183)\n",
      "Maximum depth search: 12\n",
      "Taking action right now at 2\n",
      "551 0:00:01.000229\n",
      "right: -10260.42% (-19700 / 192)\n",
      "down: -10356.45% (-19263 / 186)\n",
      "up: -10358.06% (-19266 / 186)\n",
      "left: -10358.06% (-19266 / 186)\n",
      "Maximum depth search: 7\n",
      "Taking action right now at 3\n",
      "564 0:00:01.001375\n",
      "right: -10395.31% (-19959 / 192)\n",
      "down: -10420.42% (-19903 / 191)\n",
      "left: -10460.64% (-19666 / 188)\n",
      "up: -10465.24% (-19570 / 187)\n",
      "Maximum depth search: 7\n",
      "Taking action right now at 4\n",
      "525 0:00:01.001091\n",
      "right: -10508.99% (-19862 / 189)\n",
      "down: -10511.64% (-19867 / 189)\n",
      "left: -10539.78% (-19604 / 186)\n",
      "up: -10559.46% (-19535 / 185)\n",
      "Maximum depth search: 18\n",
      "Taking action right now at 5\n",
      "547 0:00:01.001528\n",
      "down: -10592.63% (-20126 / 190)\n",
      "right: -10629.79% (-19984 / 188)\n",
      "up: -10630.32% (-19985 / 188)\n",
      "left: -10630.85% (-19986 / 188)\n",
      "Maximum depth search: 11\n",
      "Taking action down now at 14\n",
      "533 0:00:01.002243\n",
      "down: -10376.86% (-23763 / 229)\n",
      "right: -10594.29% (-22248 / 210)\n",
      "left: -10725.63% (-21344 / 199)\n",
      "up: -10735.18% (-21363 / 199)\n",
      "Maximum depth search: 15\n",
      "Taking action down now at 23\n",
      "561 0:00:01.001030\n",
      "right: -10586.73% (-23926 / 226)\n",
      "down: -10666.67% (-23360 / 219)\n",
      "up: -10839.51% (-22221 / 205)\n",
      "left: -10840.98% (-22224 / 205)\n",
      "Maximum depth search: 12\n",
      "Taking action right now at 24\n",
      "527 0:00:01.001291\n",
      "down: -10464.61% (-25429 / 243)\n",
      "up: -10930.20% (-22079 / 202)\n",
      "left: -10932.18% (-22083 / 202)\n",
      "right: -10933.33% (-21976 / 201)\n",
      "Maximum depth search: 7\n",
      "Taking action down now at 33\n",
      "516 0:00:01.001068\n",
      "right: -10409.31% (-25711 / 247)\n",
      "down: -10846.12% (-22343 / 206)\n",
      "left: -11024.87% (-21278 / 193)\n",
      "up: -11029.69% (-21177 / 192)\n",
      "Maximum depth search: 0\n",
      "Taking action right now at 34\n",
      "2028 0:00:01.001437\n",
      "right: -1044.11% (-20308 / 1945)\n",
      "down: -10983.33% (-13180 / 120)\n",
      "up: -11043.22% (-13031 / 118)\n",
      "left: -11107.76% (-12885 / 116)\n",
      "Maximum depth search: 0\n",
      "Taking action right now at 35\n",
      "17302 0:00:01.000107\n",
      "up: 805.52% (153902 / 19106)\n",
      "left: -10535.71% (-5900 / 56)\n",
      "down: -10653.70% (-5753 / 54)\n",
      "right: -10843.40% (-5747 / 53)\n",
      "Maximum depth search: 0\n",
      "Taking action up now at 26\n",
      "31804 0:00:01.000008\n",
      "up: 959.33% (486716 / 50735)\n",
      "down: -10514.52% (-6519 / 62)\n",
      "left: -10725.00% (-6435 / 60)\n",
      "right: -11045.61% (-6296 / 57)\n",
      "Maximum depth search: 0\n",
      "Taking action up now at 17\n",
      "63806 0:00:01.000008\n",
      "up: 999.99% (1143532 / 114354)\n",
      "left: -10741.54% (-6982 / 65)\n",
      "right: -10887.30% (-6859 / 63)\n",
      "down: -11077.05% (-6757 / 61)\n",
      "Maximum depth search: 0\n",
      "Taking action up now at 8\n",
      "['up', 'up', 'right', 'right', 'right', 'right', 'right', 'down', 'down', 'right', 'down', 'right', 'right', 'up', 'up', 'up']\n"
     ]
    }
   ],
   "source": [
    "gridworld = [\n",
    "       [ 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'x', 'g'],\n",
    "       [ 'o', 'o', 'x', 'o', 'o', 'o', 'o', 'x', 'o'],\n",
    "       [ 'o', 'o', 'x', 'o', 'o', 'o', 'o', 'x', 'o'],\n",
    "       [ 'o', 'o', 'x', 'o', 'o', 'o', 'o', 'o', 'o'],\n",
    "       [ 'o', 'o', 'o', 'o', 'o', 'x', 'o', 'o', 'o'],\n",
    "       [ 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o']\n",
    "    ]\n",
    "\n",
    "# \"Bill\" is the uncoopertive student\n",
    "bill = UncooperativeStudent(gridworld, 0.0)  # the second param is p(ignore advice)\n",
    "bill.world.coord_print() # print out the maze as a set of coordinates\n",
    "tree = MCTS(bill, time=1., C=200.0, max_moves = 100) # build the MCTS solver\n",
    "\n",
    "action_seq = [] # history of what i've chosen\n",
    "state = bill.start(18) # choose a start state (delete argument to make it a random valid state)\n",
    "print(\"Starting at state %s\"%state)\n",
    "\n",
    "while bill.world.gridmap_flat[state]!='g': # until we reach the goal\n",
    "    tree.update(state) # update the current state\n",
    "    action = tree.get_play() # choose the action based on planning\n",
    "    action_seq.append(action) # record the history of actions\n",
    "    state = bill.next_state(state, action) # get next state (here instructing bill to take action but he might not)\n",
    "    print(\"Taking action %s now at %s\"% (action, state)) # where i am now\n",
    "    \n",
    "print(action_seq) # print sequence of staes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
